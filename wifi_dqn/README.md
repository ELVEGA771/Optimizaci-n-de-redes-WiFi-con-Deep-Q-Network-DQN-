# Optimizaci√≥n de Redes WiFi con Deep Q-Network (DQN)

**Autores:** Jarod Tierra y Andr√©s Vega  
**NRC:** 3710

Este repositorio contiene el proyecto final para el curso de Redes, enfocado en la **Optimizaci√≥n Adaptativa de Redes WiFi mediante Reinforcement Learning (RL)**.

![Training Results](https://github.com/ELVEGA771/Optimizaci-n-de-redes-WiFi-con-Deep-Q-Network-DQN-/blob/main/output_images/training_results.png?raw=true)
*(Nota: Las im√°genes de resultados se generar√°n al ejecutar el notebook)*

## üìã Descripci√≥n

El objetivo de este proyecto es utilizar un agente de **Deep Reinforcement Learning (DQN)** para optimizar din√°micamente los par√°metros de una red WiFi (canales y potencia de transmisi√≥n) en un entorno simulado. 

A diferencia de los simuladores comerciales complejos como NS-3, este proyecto implementa un **simulador WiFi customizado en Python** que modela aspectos clave de la red como:
- Interferencia co-canal y de canal adyacente.
- P√©rdida de propagaci√≥n (Path Loss).
- C√°lculo de RSSI, Throughput, Latencia y P√©rdida de Paquetes.
- Dinamismo en la demanda de tr√°fico de los usuarios y ruido externo.

El agente aprende a maximizar una **funci√≥n de recompensa multi-objetivo** que balancea throughput, latencia, equidad (fairness) y eficiencia energ√©tica.

## üöÄ Caracter√≠sticas Principales

*   **Entorno WiFi Simulado**: Un entorno compatible con la interfaz `Gym` que simula m√∫ltiples Access Points (APs) y usuarios en un √°rea definida.
*   **Agente DQN**: Implementaci√≥n de un agente Deep Q-Network con:
    *   **Experience Replay**: Para romper la correlaci√≥n entre muestras consecutivas.
    *   **Target Network**: Para estabilizar el entrenamiento.
    *   **Epsilon-Greedy**: Estrategia de exploraci√≥n/explotaci√≥n con decaimiento.
*   **Recompensa Multi-Objetivo**: Funci√≥n de recompensa sofisticada que considera:
    *   ‚¨ÜÔ∏è Maximizaci√≥n del Throughput total.
    *   ‚¨áÔ∏è Minimizaci√≥n de Latencia y Packet Loss.
    *   üì∂ Calidad de Se√±al (RSSI).
    *   ‚öñÔ∏è Equidad entre usuarios (Jain's Fairness Index).
    *   üîã Eficiencia energ√©tica (penalizaci√≥n por potencia m√°xima innecesaria).
    *   üîÑ Estabilidad (penalizaci√≥n por cambios frecuentes de configuraci√≥n).
*   **Escenarios de Prueba**: Configuraciones predefinidas para Baja, Media y Alta congesti√≥n.
*   **Baselines de Comparaci√≥n**: Comparaci√≥n del agente DQN contra estrategias est√°ticas, aleatorias y greedy.

## üõ†Ô∏è Requisitos e Instalaci√≥n

El proyecto est√° autocontenido en un Jupyter Notebook. Para ejecutarlo, necesitas las siguientes librer√≠as de Python:

```bash
pip install tensorflow numpy matplotlib seaborn pandas scikit-learn pyyaml tqdm
```

### Tecnolog√≠as Usadas
*   **Python 3.x**
*   **TensorFlow / Keras**: Para la red neuronal del agente.
*   **NumPy & Pandas**: Procesamiento de datos y simulaci√≥n num√©rica.
*   **Matplotlib & Seaborn**: Visualizaci√≥n de m√©tricas y resultados.

##  ‚ñ∂Ô∏è C√≥mo Ejecutar

1.  Clona este repositorio:
    ```bash
    git clone https://github.com/ELVEGA771/Optimizaci-n-de-redes-WiFi-con-Deep-Q-Network-DQN-.git
    cd Optimizaci-n-de-redes-WiFi-con-Deep-Q-Network-DQN-
    ```
2.  Instala las dependencias listadas arriba.
3.  Abre el archivo `Proyecto_final.ipynb` en Jupyter Notebook, JupyterLab o Google Colab.
4.  Ejecuta todas las celdas secuencialmente para:
    *   Inicializar el simulador.
    *   Entrenar el agente DQN.
    *   Visualizar las curvas de aprendizaje.
    *   Comparar los resultados con los baselines.
    *   Ejecutar pruebas de estr√©s.

## üìÇ Estructura del Proyecto

El notebook `Proyecto_final.ipynb` est√° organizado en las siguientes secciones:

1.  **Setup e Instalaci√≥n**: Importaci√≥n de librer√≠as.
2.  **Implementaci√≥n del Entorno WiFi**: Clases `AccessPoint`, `User`, `WiFiSimulator` y `WiFiEnvironment`.
3.  **Implementaci√≥n del Agente DQN**: Clases `DQNNetwork` (Modelo Keras), `ReplayBuffer` y `DQNAgent`.
4.  **Funci√≥n de Recompensa Multi-Objetivo**: Definici√≥n de componentes de recompensa y penalizaciones.
5.  **Entrenamiento del Modelo**: Loop principal de entrenamiento con validaci√≥n peri√≥dica.
6.  **Evaluaci√≥n y Comparaci√≥n con Baselines**: Comparativa de rendimiento vs m√©todos tradicionales.
7.  **An√°lisis de Resultados**: Gr√°ficos de evoluci√≥n de m√©tricas (Throughput, Latencia, Reward).
8.  **Simulaci√≥n de Estr√©s**: Evaluaci√≥n del agente bajo condiciones extremas cambiantes.

## üìä Resultados Esperados

El entrenamiento demuestra que el agente DQN es capaz de converger a una pol√≠tica que supera a las estrategias aleatorias y est√°ticas, adapt√°ndose din√°micamente a las condiciones de interferencia y tr√°fico.

*   **Mejora de Throughput**: El agente aprende a seleccionar canales menos congestionados.
*   **Gesti√≥n de Potencia**: Ajusta la potencia para mantener cobertura sin causar interferencia excesiva.
*   **Adaptabilidad**: En las pruebas de estr√©s, el agente recupera el rendimiento ante cambios bruscos en el entorno.

---
*Este proyecto fue desarrollado como parte de la evaluaci√≥n final de la materia de Redes Teoria.*
